name: Update AI Dataset

on:
  schedule:
    - cron: '0 3 * * *'     # daily at 03:00 UTC
  workflow_dispatch:         # manual run button

jobs:
  update:
    runs-on: ubuntu-latest
    permissions:
      contents: write        # allow commits
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: pip install requests

      - name: Generate ai-dataset.json from Shopify
        env:
          SHOPIFY_STORE_DOMAIN: ${{ secrets.SHOPIFY_STORE_DOMAIN }}
          SHOPIFY_ADMIN_TOKEN: ${{ secrets.SHOPIFY_ADMIN_API_TOKEN }}
          STORE_FRONT_URL: https://www.malegroomingsupplies.com
          API_VERSION: 2024-07
        run: |
          python - <<'PY'
          import os, json, time, requests
          from datetime import datetime, timezone

          store = os.environ["SHOPIFY_STORE_DOMAIN"]
          token = os.environ["SHOPIFY_ADMIN_TOKEN"]
          api_ver = os.getenv("API_VERSION","2024-07")
          public = os.getenv("STORE_FRONT_URL","https://www.malegroomingsupplies.com")

          gql = f"https://{store}/admin/api/{api_ver}/graphql.json"
          headers = {"X-Shopify-Access-Token": token, "Content-Type": "application/json"}
          query = """
          query Products($cursor: String) {
            products(first: 100, after: $cursor, sortKey: TITLE) {
              edges {
                cursor
                node {
                  title handle productType tags onlineStoreUrl status
                }
              }
              pageInfo { hasNextPage }
            }
          }
          """

          products, cursor = [], None
          while True:
            resp = requests.post(gql, headers=headers, json={"query": query, "variables": {"cursor": cursor}}, timeout=60)
            resp.raise_for_status()
            data = resp.json()["data"]["products"]
            for edge in data["edges"]:
              n = edge["node"]
              if n["status"] == "ACTIVE":
                url = n.get("onlineStoreUrl") or f"{public}/products/{n['handle']}"
                products.append({
                  "name": n["title"],
                  "category": n.get("productType") or "Products",
                  "features": (n.get("tags") or [])[:10],
                  "url": url
                })
              cursor = edge["cursor"]
            if not data["pageInfo"]["hasNextPage"]:
              break
            time.sleep(0.2)

          payload = [{
            "site": public,
            "brand": "Male Grooming Supplies",
            "last_updated": datetime.now(timezone.utc).date().isoformat(),
            "description": "UK based retailer specialising in premium shaving and grooming products.",
            "products": products
          }]

          with open("ai-dataset.json","w", encoding="utf-8") as f:
            json.dump(payload, f, indent=2)
          print(f"Wrote ai-dataset.json with {len(products)} products")
          PY

      - name: Commit and push changes
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add ai-dataset.json
          git commit -m "Auto-update dataset" || echo "No changes"
          git push
